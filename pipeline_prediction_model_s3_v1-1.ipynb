{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c7bdd71-c11a-49a6-b372-9ee5613f6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from google.cloud import aiplatform\n",
    "from kfp.v2 import dsl, compiler\n",
    "from kfp.v2.dsl import (Artifact, ClassificationMetrics, Input, Metrics, Output, component, Dataset)\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44e57077-1655-4f4e-9356-4f506944b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\",\n",
    "        \"google-cloud-bigquery-storage\",\n",
    "        \"pandas\",\n",
    "        \"scikit-learn\",\n",
    "        \"joblib\",\n",
    "        \"db-dtypes\",\n",
    "        \"pyarrow\",\n",
    "        \"pandas-gbq\",\n",
    "        \"google-cloud-storage\",\n",
    "        \"pytz\"\n",
    "    ],\n",
    ")\n",
    "def prediction(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "):  \n",
    "    import sys\n",
    "    from datetime import datetime\n",
    "    import pandas as pd\n",
    "    from google.cloud import bigquery\n",
    "    from google.auth import default\n",
    "    import pandas_gbq\n",
    "    from google.cloud import storage\n",
    "    from joblib import load\n",
    "    from io import BytesIO\n",
    "    from pytz import timezone\n",
    "    \n",
    "    TZ = timezone('America/Lima')\n",
    "    FORMAT_DATE = \"%Y-%m-%d\"\n",
    "    \n",
    "    # Cliente BigQuery\n",
    "    client = bigquery.Client(project=project)\n",
    "    \n",
    "    # Leer datos de BigQuery\n",
    "    X_train = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=source_x_train_table)).to_dataframe()\n",
    "\n",
    "    \n",
    "    # Leer características seleccionadas de BigQuery\n",
    "    features = client.query(\n",
    "    '''SELECT * FROM `{dsource_table}`\n",
    "        '''.format(dsource_table=features_table)).to_dataframe()\n",
    "    \n",
    "    features = features[\"string_field_0\"].tolist()\n",
    "    \n",
    "\n",
    "    X_train = X_train[features]\n",
    "    \n",
    "    def generate_datetime_created():\n",
    "        return datetime.now()\n",
    "    \n",
    "    def generate_date_created():\n",
    "        return datetime.now(TZ).date().strftime(FORMAT_DATE)\n",
    "    \n",
    "    \n",
    "    def load_model_from_gcs(path_model):\n",
    "        # Inicializar el cliente de Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Obtener el nombre del bucket y la ruta del objeto\n",
    "        bucket_name, blob_name = path_model.replace(\"gs://\", \"\").split(\"/\", 1)\n",
    "\n",
    "        # Obtener el objeto desde Cloud Storage\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name)\n",
    "        model_bytes = blob.download_as_string()\n",
    "\n",
    "        # Cargar el modelo desde los bytes obtenidos\n",
    "        classifier = load(BytesIO(model_bytes))\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    classifier = load_model_from_gcs(path_model)\n",
    "\n",
    "    # Realizar la predicción\n",
    "    predictions = classifier.predict(X_train)\n",
    "    predictions = pd.DataFrame(predictions, columns=['prediction'])\n",
    "\n",
    "    # Obtener el user_id de la sesión actual en BigQuery\n",
    "    user_id = client.query(\"SELECT SESSION_USER()\").to_dataframe().iloc[0, 0]\n",
    "\n",
    "    # Agregar campos de auditoría\n",
    "    start_time = generate_datetime_created()\n",
    "    execute_date = generate_date_created()\n",
    "    \n",
    "    predictions['creation_user'] = user_id\n",
    "    predictions['process_date'] = datetime.strptime(execute_date, '%Y-%m-%d')\n",
    "    predictions['process_date'] = pd.to_datetime(predictions['process_date']).dt.date\n",
    "    predictions['load_date'] = pd.to_datetime(start_time)\n",
    "    \n",
    "    # Guardar el resultado en BigQuery \n",
    "    pandas_gbq.to_gbq(predictions , table_id, if_exists='append', project_id=project)\n",
    "\n",
    "    print(\"Predicción generada y guardada en BigQuery.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d6605f-8ca6-4287-bc46-7ad62a8cabd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name=\"pipeline-prediction-model\", \n",
    "    description=\"intro\",\n",
    "    pipeline_root=\"gs://<bucket>/demo\"\n",
    ")\n",
    "\n",
    "def main_pipeline(\n",
    "    project: str,\n",
    "    source_x_train_table: str,\n",
    "    features_table: str,\n",
    "    table_id: str,\n",
    "    path_model: str,\n",
    "    gcp_region: str = \"us-central1\",\n",
    "):\n",
    "\n",
    "    prediction_tast = prediction(\n",
    "        project = project,\n",
    "        source_x_train_table = source_x_train_table,\n",
    "        features_table = features_table,\n",
    "        table_id = table_id,\n",
    "        path_model = path_model,\n",
    "    )\n",
    "    prediction_tast.set_display_name(\"PREDICTION_MODEL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a09581f-2348-45b5-8533-225e3d04d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=main_pipeline,\n",
    "    package_path=\"pipeline_prediction.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dd4e0a1-9ca9-4a10-964d-a2a9469af54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo pipeline_prediction.json subido a demo/pipeline_prediction.json en el bucket dev-dp-vertex-analytics-ai.\n"
     ]
    }
   ],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_name, destination_blob_name):\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "    print(f\"Archivo {source_file_name} subido a {destination_blob_name} en el bucket {bucket_name}.\")\n",
    "\n",
    "# Define las variables\n",
    "bucket_name = \"<bucket>\"\n",
    "destination_blob_name = \"demo/pipeline_prediction.json\"\n",
    "pipeline_file = \"pipeline_prediction.json\"\n",
    "# Llamar a la función para subir el archivo\n",
    "upload_to_gcs(bucket_name, pipeline_file, destination_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d30531d7-f798-420e-9cde-d8bd781deb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=\"<project_id>\", location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "717e7da3-41a5-4a91-b763-a36acfdc6c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submit pipeline job ...\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/624205664083/locations/us-central1/pipelineJobs/intro-20240602053404\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/624205664083/locations/us-central1/pipelineJobs/intro-20240602053404')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/intro-20240602053404?project=624205664083\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"pipeline de prueba\",\n",
    "    template_path=\"gs://<bucket>/demo/pipeline_prediction.json\",\n",
    "    enable_caching=False,\n",
    "    project=\"<project_id>\",\n",
    "    location=\"us-central1\",\n",
    "    parameter_values={\"project\": \"<project_id>\", \n",
    "                      \"source_x_train_table\": \"<project_id>.<dataset>.xtrain\",\n",
    "                      \"features_table\": \"<project_id>.<dataset>.selected_features\",\n",
    "                      \"table_id\": \"<project_id>.<dataset>.predictions\",\n",
    "                      \"path_model\": \"gs://<bucket>/demo/data/model/model.joblib\"\n",
    "                     }\n",
    "    #labels={\"module\": \"ml\", \"application\": \"app\", \"chapter\": \"mlops\", \"company\": \"datapat\", \"environment\": \"dev\", \"owner\": \"xxxx\"}\n",
    ")\n",
    "\n",
    "print('submit pipeline job ...')\n",
    "job.submit(service_account=\"dev-dp-ml-vertex@<project_id>.iam.gserviceaccount.com\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
